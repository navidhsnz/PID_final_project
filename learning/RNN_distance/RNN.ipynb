{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneDetectionCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 224, 224)):\n",
    "        super(LaneDetectionCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self._to_linear = None\n",
    "        self._calculate_flat_size(input_shape)\n",
    "\n",
    "    def _calculate_flat_size(self, input_shape):\n",
    "        \"\"\"Pass a dummy tensor through the convolutional layers to determine the flattened size.\"\"\"\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        x = self._forward_conv(x)\n",
    "        self._to_linear = x.numel()\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.maxpool3(x)\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for RNN input\n",
    "        return x\n",
    "\n",
    "\n",
    "class LaneDetectionRNN(nn.Module):\n",
    "    def __init__(self, input_shape, rnn_hidden_size=128, num_frequencies=6):\n",
    "        super(LaneDetectionRNN, self).__init__()\n",
    "        self.cnn = LaneDetectionCNN(input_shape)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.cnn._to_linear + 2 * 2 * num_frequencies, \n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(rnn_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, actions, hidden_state=None):\n",
    "        batch_size, seq_length, channels, height, width = x.size()\n",
    "        x = x.view(batch_size * seq_length, channels, height, width)\n",
    "        cnn_features = self.cnn(x)\n",
    "        cnn_features = cnn_features.view(batch_size, seq_length, -1)\n",
    "\n",
    "        # Shift the actions so for each image, the action that goes with it as input is that of the time step of the previous image.\n",
    "        shifted_actions = torch.zeros_like(actions) \n",
    "        shifted_actions[:, 1:] = actions[:, :-1]  # Shift actions by one timestep\n",
    "        shifted_actions[:, 0] = 0.0\n",
    "\n",
    "        rnn_input = torch.cat((cnn_features, shifted_actions), dim=-1)  # (batch_size, seq_length, feature_size)\n",
    "\n",
    "        # Process with RNN\n",
    "        rnn_out, hidden_state = self.rnn(rnn_input, hidden_state)\n",
    "        predictions = torch.tanh(self.fc(rnn_out))\n",
    "        return predictions, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def apply_preprocessing(image):\n",
    "    \"\"\"\n",
    "    Apply preprocessing transformations to the input image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object.\n",
    "    \"\"\"\n",
    "    image_array = np.array(image)\n",
    "    channels = [image_array[:, :, i] for i in range(image_array.shape[2])]\n",
    "    h, w, _ = image_array.shape\n",
    "    \n",
    "    imghsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)\n",
    "    img = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    mask_ground = np.ones(img.shape, dtype=np.uint8)  # Start with a mask of ones (white)\n",
    "\n",
    "\n",
    "    one_third_height = h // 3\n",
    "    mask_ground[:one_third_height, :] = 0  # Mask the top 1/3 of the image\n",
    "    \n",
    "    #gaussian filter\n",
    "    sigma = 4.5\n",
    "    img_gaussian_filter = cv2.GaussianBlur(img,(0,0), sigma)\n",
    "    \n",
    "    sobelx = cv2.Sobel(img_gaussian_filter,cv2.CV_64F,1,0)\n",
    "    sobely = cv2.Sobel(img_gaussian_filter,cv2.CV_64F,0,1)\n",
    "    # Compute the magnitude of the gradients\n",
    "    Gmag = np.sqrt(sobelx*sobelx + sobely*sobely)\n",
    "    threshold = 50\n",
    "\n",
    "\n",
    "    white_lower_hsv = np.array([0, 0, 153])         # CHANGE ME\n",
    "    white_upper_hsv = np.array([228, 69, 255])   # CHANGE ME\n",
    "    yellow_lower_hsv = np.array([15, 30, 100])        # CHANGE ME\n",
    "    yellow_upper_hsv = np.array([35, 254, 255])  # CHANGE ME\n",
    "\n",
    "    mask_white = cv2.inRange(imghsv, white_lower_hsv, white_upper_hsv)\n",
    "    mask_yellow = cv2.inRange(imghsv, yellow_lower_hsv, yellow_upper_hsv)\n",
    "\n",
    "\n",
    "    mask_mag = (Gmag > threshold)\n",
    "\n",
    "    # np.savetxt(\"mask.txt\", mask_white, fmt='%d', delimiter=',')\n",
    "    # exit()\n",
    "\n",
    "    final_mask = mask_ground * mask_mag * 255 \n",
    "    mask_white = mask_ground * mask_white\n",
    "    mask_yellow = mask_ground * mask_yellow\n",
    "    # Convert the NumPy array back to a PIL image\n",
    "\n",
    "    channels[0] =  final_mask\n",
    "    channels[1] =  mask_white\n",
    "    channels[2] =  mask_yellow\n",
    "    \n",
    "    filtered_image = np.stack(channels, axis=-1)\n",
    "    filtered_image = Image.fromarray(filtered_image)\n",
    "    return  filtered_image\n",
    "\n",
    "\n",
    "class SequentialImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, action_folder, seq_length=100, transform=None, num_frequencies=6):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.action_folder = action_folder\n",
    "        self.seq_length = seq_length\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load and sort\n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.label_files = sorted(os.listdir(label_folder))\n",
    "        self.action_files = sorted(os.listdir(action_folder))\n",
    "\n",
    "        # mkae sure  dataset size is divisible by seq_length\n",
    "        self.num_sequences = len(self.image_files) // seq_length\n",
    "        self.image_files = self.image_files[:self.num_sequences * seq_length]\n",
    "        self.label_files = self.label_files[:self.num_sequences * seq_length]\n",
    "        self.action_files = self.action_files[:self.num_sequences * seq_length]\n",
    "\n",
    "        self.sequences = [\n",
    "            (self.image_files[i:i + seq_length],\n",
    "             self.label_files[i:i + seq_length],\n",
    "             self.action_files[i:i + seq_length])\n",
    "            for i in range(0, len(self.image_files), seq_length)\n",
    "        ]\n",
    "        random.shuffle(self.sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_sequence = []\n",
    "        label_sequence = []\n",
    "        action_sequence = []\n",
    "\n",
    "        image_files, label_files, action_files = self.sequences[idx]\n",
    "        for img_file, lbl_file, act_file in zip(image_files, label_files, action_files):\n",
    "            # load and transform image\n",
    "            img_path = os.path.join(self.image_folder, img_file)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            image_sequence.append(image)\n",
    "\n",
    "            # Load label\n",
    "            lbl_path = os.path.join(self.label_folder, lbl_file)\n",
    "            with open(lbl_path, \"r\") as f:\n",
    "                label = float(f.read().strip())\n",
    "            label_sequence.append(label)\n",
    "\n",
    "            # Load actions\n",
    "            act_path = os.path.join(self.action_folder, act_file)\n",
    "            with open(act_path, \"r\") as f:\n",
    "                speed, angular_velocity = map(float, f.read().strip().split())\n",
    "\n",
    "                # Compute Fourier features\n",
    "                speed_features = compute_fourier_features(torch.tensor(speed), self.num_frequencies)\n",
    "                angular_features = compute_fourier_features(torch.tensor(angular_velocity), self.num_frequencies)\n",
    "\n",
    "                action_sequence.append(torch.cat([speed_features, angular_features]))\n",
    "\n",
    "\n",
    "        image_sequence = torch.stack(image_sequence) \n",
    "        label_sequence = torch.tensor(label_sequence, dtype=torch.float32)\n",
    "        action_sequence = torch.stack(action_sequence)\n",
    "        return image_sequence, label_sequence, action_sequence\n",
    "\n",
    "def compute_fourier_features(value, num_frequencies=6):\n",
    "    frequencies = 2 ** torch.arange(num_frequencies, dtype=torch.float32)\n",
    "    features = torch.cat([\n",
    "        torch.sin(frequencies * value),\n",
    "        torch.cos(frequencies * value)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "def get_sequential_dataloader(\n",
    "    image_folder, label_folder, action_folder, batch_size, seq_length=100, train_fraction=0.8, val_fraction=0.1, test_fraction=0.1\n",
    "):\n",
    "    if not (0.0 < train_fraction + val_fraction + test_fraction <= 1.0):\n",
    "        raise ValueError(\"Fractions for train, validation, and test must sum to 1.0.\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(apply_preprocessing),\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "\n",
    "    # load the dataset\n",
    "    dataset = SequentialImageDataset(image_folder, label_folder, action_folder, seq_length=seq_length, transform=transform)\n",
    "\n",
    "    # Compute sizes for train validation and test\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_fraction)\n",
    "    val_size = int(total_size * val_fraction)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # split\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader # return the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function evaluates the model\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, actions in dataloader:\n",
    "            images, labels, actions = images.to(device), labels.to(device), actions.to(device)\n",
    "\n",
    "            predictions,_ = model(images, actions)\n",
    "            predictions = predictions.squeeze(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# training function\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, n_epochs=10\n",
    "):\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store loss values\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for images, labels, actions in tqdm(train_loader, total=len(train_loader),\n",
    "                                                desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\"):\n",
    "\n",
    "            images, labels, actions = images.to(device), labels.to(device), actions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions, _ = model(images, actions)\n",
    "            predictions = predictions.squeeze(-1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # validation\n",
    "        avg_val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{n_epochs}] - \"\n",
    "            f\"Train Loss: {avg_train_loss:.7f}, Validation Loss: {avg_val_loss:.7f}\"\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Dataset paths\n",
    "    IMAGE_FOLDER = \"training_images/trail2/images\"\n",
    "    LABEL_FOLDER = \"training_images/trail2/labels\"\n",
    "    ACTION_FOLDER = \"training_images/trail2/actions\"\n",
    "\n",
    "    batch_size = 10  # number of sequences per batch\n",
    "    seq_length = 20  # number of images per sequence\n",
    "    n_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Define splits\n",
    "    train_fraction = 0.85 \n",
    "    val_fraction = 0.1 \n",
    "    test_fraction = 0.05\n",
    "\n",
    "    # create DataLoaders\n",
    "    train_loader, val_loader, test_loader = get_sequential_dataloader(\n",
    "        IMAGE_FOLDER, LABEL_FOLDER, ACTION_FOLDER, batch_size=batch_size, seq_length=seq_length,\n",
    "        train_fraction=train_fraction, val_fraction=val_fraction, test_fraction=test_fraction\n",
    "    )\n",
    "\n",
    "    # Initialize\n",
    "    input_shape = (3, 480, 640)\n",
    "    model = LaneDetectionRNN(input_shape=input_shape, rnn_hidden_size=128)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train the model with validation\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, n_epochs)\n",
    "\n",
    "    # Save the model for ftutre\n",
    "    torch.save(model.state_dict(), \"models/lane_detection_rnn8_pp.pth\")\n",
    "    print(\"Model saved to 'lane_detection_rnn8_pp.pth'\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss = validate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded successfully from 'models/lane_detection_rnn8_pp.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167116/1023512389.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# loading saved model\n",
    "\n",
    "def load_model(model_class, model_path, device, input_shape, rnn_hidden_size=128):\n",
    "    model = model_class(input_shape=input_shape, rnn_hidden_size=rnn_hidden_size)\n",
    "    \n",
    "    # Load state dictionary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(f\"Model loaded successfully from '{model_path}'\")\n",
    "    return model\n",
    "\n",
    "# Dataset paths\n",
    "IMAGE_FOLDER = \"training_images/trail2/images\"\n",
    "LABEL_FOLDER = \"training_images/trail2/labels\"\n",
    "ACTION_FOLDER = \"training_images/trail2/actions\"\n",
    "MODEL_PATH = \"models/lane_detection_rnn8_pp.pth\"\n",
    "\n",
    "batch_size = 4  # Number of sequences per batch\n",
    "seq_length = 100  # Number of images per sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create DataLoader for testing\n",
    "_, _, test_loader = get_sequential_dataloader(\n",
    "    IMAGE_FOLDER, LABEL_FOLDER, ACTION_FOLDER, batch_size=batch_size, seq_length=seq_length,\n",
    "    train_fraction=0.8, val_fraction=0.1, test_fraction=0.1\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "input_shape = (3, 480, 640)  # Input shape of images (channels, height, width)\n",
    "model = load_model(LaneDetectionRNN, MODEL_PATH, device, input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0017802\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()  # Mean squared error loss\n",
    "\n",
    "test_loss = validate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def visualize_predictions(model, dataloader, device, num_examples=3, seq_length=10):\n",
    "    \"\"\"\n",
    "    Visualize predictions vs. real labels for a few examples from the dataset using OpenCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained RNN model.\n",
    "    - dataloader: DataLoader providing the dataset.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "    - num_examples: Number of examples to visualize.\n",
    "    - seq_length: Number of consecutive images in each sequence.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    examples_shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, actions in dataloader:\n",
    "            # Ensure images and labels are on the correct device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            actions = actions.to(device)\n",
    "            \n",
    "            # Get predictions for the batch\n",
    "            predictions, _ = model(images, actions)  # Shape: (batch_size, seq_length)\n",
    "            predictions = predictions.squeeze(-1)\n",
    "            # Iterate through the batch\n",
    "            for i in range(len(images)):\n",
    "                if examples_shown >= num_examples:\n",
    "                    return  # Stop after showing the required number of examples\n",
    "                \n",
    "                # Extract the sequence and predictions\n",
    "                image_sequence = images[i][:seq_length].cpu()  # Take the first `seq_length` images\n",
    "                real_labels = labels[i][:seq_length].cpu().numpy()\n",
    "                predicted_labels = predictions[i][:seq_length].cpu().numpy()\n",
    "                \n",
    "                # Prepare OpenCV visualization\n",
    "                for t in range(seq_length):\n",
    "                    # Convert image tensor to a format OpenCV can display\n",
    "                    image = (image_sequence[t].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "                    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "\n",
    "                    # Overlay predictions and real labels\n",
    "                    text = f\"P: {predicted_labels[t]}\\nR: {real_labels[t]}\"\n",
    "                    cv2.putText(image_bgr, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                    # Display the image\n",
    "                    cv2.imshow(f\"Sequence {examples_shown + 1} - Frame {t + 1}/{seq_length}\", image_bgr)\n",
    "                    cv2.waitKey(20000)  # Wait 500ms between frames\n",
    "\n",
    "                cv2.destroyAllWindows()\n",
    "                examples_shown += 1\n",
    "\n",
    "visualize_predictions(model, test_loader, device, num_examples=3, seq_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
