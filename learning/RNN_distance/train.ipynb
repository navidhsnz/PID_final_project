{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneDetectionCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 224, 224)):\n",
    "        super(LaneDetectionCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Calculate flat size dynamically for compatibility with RNNs\n",
    "        self._to_linear = None\n",
    "        self._calculate_flat_size(input_shape)\n",
    "\n",
    "    def _calculate_flat_size(self, input_shape):\n",
    "        \"\"\"Pass a dummy tensor through the convolutional layers to determine the flattened size.\"\"\"\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        x = self._forward_conv(x)\n",
    "        self._to_linear = x.numel()\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        \"\"\"Forward pass through convolutional layers.\"\"\"\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.maxpool3(x)\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the CNN.\n",
    "        Returns flattened features suitable for RNN input.\n",
    "        \"\"\"\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for RNN input\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneDetectionRNN(nn.Module):\n",
    "    def __init__(self, input_shape, rnn_hidden_size=128, action_embedding_size=32, num_frequencies=6):\n",
    "        super(LaneDetectionRNN, self).__init__()\n",
    "        self.cnn = LaneDetectionCNN(input_shape)\n",
    "        # self.action_embed = nn.Linear(2, action_embedding_size)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.cnn._to_linear + 2 * 2 * num_frequencies,  # Feature size from CNN\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(rnn_hidden_size, 1)  # Predict distance for each timestep\n",
    "\n",
    "    def forward(self, x, actions):\n",
    "        batch_size, seq_length, channels, height, width = x.size()\n",
    "        x = x.view(batch_size * seq_length, channels, height, width)  # Flatten sequence dimension\n",
    "        cnn_features = self.cnn(x)  # Extract features\n",
    "        cnn_features = cnn_features.view(batch_size, seq_length, -1)  # Restore sequence dimension\n",
    "\n",
    "        # # Process actions\n",
    "        # actions = self.action_embed(actions) # Shape: (batch_size, seq_length, action_embedding_size)\n",
    "\n",
    "        # Concatenate CNN features and action embeddings\n",
    "        rnn_input = torch.cat((cnn_features, actions), dim=-1) # Shape: (batch_size, seq_length, feature_size)\n",
    "\n",
    "\n",
    "\n",
    "        rnn_out, _ = self.rnn(rnn_input)  # Process with RNN\n",
    "        predictions = torch.tanh(self.fc(rnn_out))  # Predict for each timestep\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "class SequentialImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, action_folder, seq_length=100, transform=None, num_frequencies=6):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.action_folder = action_folder\n",
    "        self.seq_length = seq_length\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load and sort image, label, and action files\n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.label_files = sorted(os.listdir(label_folder))\n",
    "        self.action_files = sorted(os.listdir(action_folder))\n",
    "\n",
    "        # Ensure dataset size is divisible by seq_length\n",
    "        self.num_sequences = len(self.image_files) // seq_length\n",
    "        self.image_files = self.image_files[:self.num_sequences * seq_length]\n",
    "        self.label_files = self.label_files[:self.num_sequences * seq_length]\n",
    "        self.action_files = self.action_files[:self.num_sequences * seq_length]\n",
    "\n",
    "        # Split into sequences\n",
    "        self.sequences = [\n",
    "            (self.image_files[i:i + seq_length],\n",
    "             self.label_files[i:i + seq_length],\n",
    "             self.action_files[i:i + seq_length])\n",
    "            for i in range(0, len(self.image_files), seq_length)\n",
    "        ]\n",
    "        random.shuffle(self.sequences)  # Shuffle sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_sequence = []\n",
    "        label_sequence = []\n",
    "        action_sequence = []\n",
    "\n",
    "        image_files, label_files, action_files = self.sequences[idx]\n",
    "        for img_file, lbl_file, act_file in zip(image_files, label_files, action_files):\n",
    "            # Load and transform image\n",
    "            img_path = os.path.join(self.image_folder, img_file)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            image_sequence.append(image)\n",
    "\n",
    "            # Load label\n",
    "            lbl_path = os.path.join(self.label_folder, lbl_file)\n",
    "            with open(lbl_path, \"r\") as f:\n",
    "                label = float(f.read().strip())\n",
    "            label_sequence.append(label)\n",
    "\n",
    "            # Load action (speed and angular velocity)\n",
    "            act_path = os.path.join(self.action_folder, act_file)\n",
    "            with open(act_path, \"r\") as f:\n",
    "                speed, angular_velocity = map(float, f.read().strip().split())\n",
    "                \n",
    "                # Compute Fourier features\n",
    "                speed_features = compute_fourier_features(torch.tensor(speed), self.num_frequencies)\n",
    "                angular_features = compute_fourier_features(torch.tensor(angular_velocity), self.num_frequencies)\n",
    "                \n",
    "                # Concatenate speed and angular features\n",
    "                action_sequence.append(torch.cat([speed_features, angular_features]))\n",
    "\n",
    "\n",
    "        image_sequence = torch.stack(image_sequence)  # Shape: (seq_length, channels, height, width)\n",
    "        label_sequence = torch.tensor(label_sequence, dtype=torch.float32)  # Shape: (seq_length,)\n",
    "        action_sequence = torch.stack(action_sequence)\n",
    "        return image_sequence, label_sequence, action_sequence\n",
    "    \n",
    "def compute_fourier_features(value, num_frequencies=6):\n",
    "    \"\"\"\n",
    "    Compute sine-cosine Fourier features for the input value.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: Input scalar (e.g., speed or angular velocity).\n",
    "    - num_frequencies: Number of frequency terms to include.\n",
    "    \n",
    "    Returns:\n",
    "    - features: Tensor of shape (2 * num_frequencies,).\n",
    "    \"\"\"\n",
    "    frequencies = 2 ** torch.arange(num_frequencies, dtype=torch.float32)  # Exponentially increasing frequencies\n",
    "    features = torch.cat([\n",
    "        torch.sin(frequencies * value),\n",
    "        torch.cos(frequencies * value)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "def get_sequential_dataloader(\n",
    "    image_folder, label_folder, action_folder, batch_size, seq_length=100, train_fraction=0.8, val_fraction=0.1, test_fraction=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create DataLoaders for training, validation, and testing datasets, ensuring distinct splits.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: Path to the folder containing images.\n",
    "    - label_folder: Path to the folder containing labels.\n",
    "    - batch_size: Number of sequences per batch.\n",
    "    - seq_length: Number of images per sequence.\n",
    "    - train_fraction: Fraction of the data to use for training.\n",
    "    - val_fraction: Fraction of the data to use for validation.\n",
    "    - test_fraction: Fraction of the data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "    - train_loader: DataLoader for training.\n",
    "    - val_loader: DataLoader for validation.\n",
    "    - test_loader: DataLoader for testing.\n",
    "    \"\"\"\n",
    "    if not (0.0 < train_fraction + val_fraction + test_fraction <= 1.0):\n",
    "        raise ValueError(\"Fractions for train, validation, and test must sum to 1.0 or less.\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "\n",
    "    # Load the full dataset\n",
    "    dataset = SequentialImageDataset(image_folder, label_folder, action_folder, seq_length=seq_length, transform=transform)\n",
    "\n",
    "    # Compute sizes for train, validation, and test splits\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_fraction)\n",
    "    val_size = int(total_size * val_fraction)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Perform the splits\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Train DataLoader:\n",
      "Image batch shape: torch.Size([4, 100, 3, 480, 640])\n",
      "Label batch shape: torch.Size([4, 100])\n",
      "Label batch shape: torch.Size([4, 100, 24])\n",
      "\n",
      "Testing Validation DataLoader:\n",
      "Image batch shape: torch.Size([4, 100, 3, 480, 640])\n",
      "Label batch shape: torch.Size([4, 100])\n",
      "Label batch shape: torch.Size([4, 100, 24])\n",
      "\n",
      "Testing Test DataLoader:\n",
      "Image batch shape: torch.Size([4, 100, 3, 480, 640])\n",
      "Label batch shape: torch.Size([4, 100])\n",
      "Label batch shape: torch.Size([4, 100, 24])\n",
      "number of data:  [35, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    IMAGE_FOLDER = \"training_images/trail2/images\"\n",
    "    LABEL_FOLDER = \"training_images/trail2/labels\"\n",
    "    ACTION_FOLDER = \"training_images/trail2/actions\"\n",
    "    batch_size = 4  # Number of sequences per batch\n",
    "    seq_length = 100  # Number of images per sequence\n",
    "\n",
    "    # Define split fractions\n",
    "    train_fraction = 0.8\n",
    "    val_fraction = 0.1\n",
    "    test_fraction = 0.1\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader, val_loader, test_loader = get_sequential_dataloader(\n",
    "        IMAGE_FOLDER, LABEL_FOLDER, ACTION_FOLDER, batch_size, seq_length,\n",
    "        train_fraction=train_fraction, val_fraction=val_fraction, test_fraction=test_fraction\n",
    "    )\n",
    "\n",
    "    # Test the training DataLoader\n",
    "    print(\"Testing Train DataLoader:\")\n",
    "    for images, labels, actions in train_loader:\n",
    "        print(f\"Image batch shape: {images.shape}\")  # Expected: (batch_size, seq_length, channels, height, width)\n",
    "        print(f\"Label batch shape: {labels.shape}\")  # Expected: (batch_size, seq_length)\n",
    "        print(f\"Label batch shape: {actions.shape}\")\n",
    "\n",
    "        break\n",
    "\n",
    "    # Test the validation DataLoader\n",
    "    print(\"\\nTesting Validation DataLoader:\")\n",
    "    for images, labels, actions in val_loader:\n",
    "        print(f\"Image batch shape: {images.shape}\")  # Expected: (batch_size, seq_length, channels, height, width)\n",
    "        print(f\"Label batch shape: {labels.shape}\")  # Expected: (batch_size, seq_length)\n",
    "        print(f\"Label batch shape: {actions.shape}\") \n",
    "        break\n",
    "\n",
    "    # Test the test DataLoader\n",
    "    print(\"\\nTesting Test DataLoader:\")\n",
    "    for images, labels, actions in test_loader:\n",
    "        print(f\"Image batch shape: {images.shape}\")  # Expected: (batch_size, seq_length, channels, height, width)\n",
    "        print(f\"Label batch shape: {labels.shape}\")  # Expected: (batch_size, seq_length)\n",
    "        print(f\"Label batch shape: {actions.shape}\") \n",
    "        break\n",
    "print(\"number of data: \",list(map(len, [train_loader, val_loader, test_loader])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the following classes and functions are already defined:\n",
    "# - LaneDetectionRNN\n",
    "# - SequentialImageDataset\n",
    "\n",
    "# -------------------\n",
    "# Validation Function\n",
    "# -------------------\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the LaneDetectionRNN model on the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The RNN model to validate.\n",
    "    - dataloader: DataLoader providing validation data.\n",
    "    - criterion: Loss function (e.g., MSELoss).\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, actions in dataloader:\n",
    "            images, labels, actions = images.to(device), labels.to(device), actions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images, actions)  # Shape: (batch_size, seq_length)\n",
    "            predictions = predictions.squeeze(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Training Function with Validation and Loss Tracking\n",
    "# -------------------\n",
    "def train_model_with_validation(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, n_epochs=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the LaneDetectionRNN model with validation after each epoch and track loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The RNN model to train.\n",
    "    - train_loader: DataLoader providing training data.\n",
    "    - val_loader: DataLoader providing validation data.\n",
    "    - criterion: Loss function (e.g., MSELoss).\n",
    "    - optimizer: Optimizer (e.g., Adam).\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "    - n_epochs: Number of epochs to train for.\n",
    "\n",
    "    Returns:\n",
    "    - train_losses: List of average training losses for each epoch.\n",
    "    - val_losses: List of average validation losses for each epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store loss values\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for images, labels, actions in tqdm(train_loader, total=len(train_loader),\n",
    "                                                desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\"):\n",
    "\n",
    "            images, labels, actions = images.to(device), labels.to(device), actions.to(device)\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images, actions)\n",
    "            predictions = predictions.squeeze(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{n_epochs}] - \"\n",
    "            f\"Train Loss: {avg_train_loss:.7f}, Validation Loss: {avg_val_loss:.7f}\"\n",
    "        )\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# TRAIN\n",
    "# -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Dataset paths\n",
    "    IMAGE_FOLDER = \"training_images/trail2/images\"\n",
    "    LABEL_FOLDER = \"training_images/trail2/labels\"\n",
    "    ACTION_FOLDER = \"training_images/trail2/actions\"\n",
    "\n",
    "    batch_size = 10  # Number of sequences per batch\n",
    "    seq_length = 20  # Number of images per sequence\n",
    "    n_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Define split fractions\n",
    "    train_fraction = 0.85  # 80% for training\n",
    "    val_fraction = 0.1    # 10% for validation\n",
    "    test_fraction = 0.05   # 10% for testing\n",
    "\n",
    "    # Create DataLoaders using the improved get_sequential_dataloader function\n",
    "    train_loader, val_loader, test_loader = get_sequential_dataloader(\n",
    "        IMAGE_FOLDER, LABEL_FOLDER, ACTION_FOLDER, batch_size=batch_size, seq_length=seq_length,\n",
    "        train_fraction=train_fraction, val_fraction=val_fraction, test_fraction=test_fraction\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    input_shape = (3, 480, 640)  # Update based on actual image dimensions\n",
    "    model = LaneDetectionRNN(input_shape=input_shape, rnn_hidden_size=128)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Mean squared error loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Select device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train the model with validation\n",
    "    train_losses, val_losses = train_model_with_validation(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, n_epochs\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"models/lane_detection_rnn4.pth\")\n",
    "    print(\"Model saved to 'lane_detection_rnn4.pth'\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss = validate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded successfully from 'models/lane_detection_rnn4.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37373/1437864165.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# loading saved model\n",
    "\n",
    "def load_model(model_class, model_path, device, input_shape, rnn_hidden_size=128):\n",
    "    model = model_class(input_shape=input_shape, rnn_hidden_size=rnn_hidden_size)\n",
    "    \n",
    "    # Load state dictionary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(f\"Model loaded successfully from '{model_path}'\")\n",
    "    return model\n",
    "\n",
    "# Dataset paths\n",
    "IMAGE_FOLDER = \"training_images/trail2/images\"\n",
    "LABEL_FOLDER = \"training_images/trail2/labels\"\n",
    "ACTION_FOLDER = \"training_images/trail2/actions\"\n",
    "MODEL_PATH = \"models/lane_detection_rnn4.pth\"\n",
    "\n",
    "batch_size = 4  # Number of sequences per batch\n",
    "seq_length = 100  # Number of images per sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create DataLoader for testing\n",
    "_, _, test_loader = get_sequential_dataloader(\n",
    "    IMAGE_FOLDER, LABEL_FOLDER, ACTION_FOLDER, batch_size=batch_size, seq_length=seq_length,\n",
    "    train_fraction=0.8, val_fraction=0.1, test_fraction=0.1\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "input_shape = (3, 480, 640)  # Input shape of images (channels, height, width)\n",
    "model = load_model(LaneDetectionRNN, MODEL_PATH, device, input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0001931\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()  # Mean squared error loss\n",
    "\n",
    "test_loss = validate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def visualize_predictions(model, dataloader, device, num_examples=3, seq_length=10):\n",
    "    \"\"\"\n",
    "    Visualize predictions vs. real labels for a few examples from the dataset using OpenCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained RNN model.\n",
    "    - dataloader: DataLoader providing the dataset.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "    - num_examples: Number of examples to visualize.\n",
    "    - seq_length: Number of consecutive images in each sequence.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    examples_shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, actions in dataloader:\n",
    "            # Ensure images and labels are on the correct device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            actions = actions.to(device)\n",
    "            \n",
    "            # Get predictions for the batch\n",
    "            predictions = model(images, actions).squeeze(-1)  # Shape: (batch_size, seq_length)\n",
    "            \n",
    "            # Iterate through the batch\n",
    "            for i in range(len(images)):\n",
    "                if examples_shown >= num_examples:\n",
    "                    return  # Stop after showing the required number of examples\n",
    "                \n",
    "                # Extract the sequence and predictions\n",
    "                image_sequence = images[i][:seq_length].cpu()  # Take the first `seq_length` images\n",
    "                real_labels = labels[i][:seq_length].cpu().numpy()\n",
    "                predicted_labels = predictions[i][:seq_length].cpu().numpy()\n",
    "                \n",
    "                # Prepare OpenCV visualization\n",
    "                for t in range(seq_length):\n",
    "                    # Convert image tensor to a format OpenCV can display\n",
    "                    image = (image_sequence[t].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "                    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "\n",
    "                    # Overlay predictions and real labels\n",
    "                    text = f\"P: {predicted_labels[t]}\\nR: {real_labels[t]}\"\n",
    "                    cv2.putText(image_bgr, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                    # Display the image\n",
    "                    cv2.imshow(f\"Sequence {examples_shown + 1} - Frame {t + 1}/{seq_length}\", image_bgr)\n",
    "                    cv2.waitKey(20000)  # Wait 500ms between frames\n",
    "\n",
    "                cv2.destroyAllWindows()\n",
    "                examples_shown += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "# -------------------\n",
    "# Main Script\n",
    "# -------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Visualize predictions\n",
    "    visualize_predictions(model, test_loader, device, num_examples=3, seq_length=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
